name: Micro-benchmark build
on:
  workflow_dispatch:
  pull_request:
    branches: [ master ]
    types: [opened, labeled, synchronize]
  push:
    branches:
      - 'master'
    tags:
      - 'v*'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  haslabel:
    name: Check label or branch
    runs-on: ubuntu-latest
    outputs:
      has-benchmark-label: ${{ steps.haslabel.outputs.result }}
    steps:
      - name: Check label or branch
        id: haslabel
        uses: actions/github-script@v8
        with:
          script: |
            if (context.eventName !== 'pull_request') {
              console.log("Not a pull request")
              return false
            }

            const existing_pull = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.payload.pull_request.number,
              headers: { 'Accept': 'application/vnd.github-json' }
            });


            const has_label = existing_pull.data.labels.find(label => label.name === 'micro_benchmark')
            console.log(`Found label: ${has_label !== undefined}`)
            return has_label !== undefined

  create-runner:
    needs: haslabel
    if: ${{ needs.haslabel.outputs.has-benchmark-label == 'true' || github.event_name != 'pull_request' }}
    runs-on: ubuntu-latest
    steps:
      - name: Create Runner For Micro-Benchmark
        id: create-runner
        uses: FalkorDB/gce-github-runner@install_docker
        with:
          token: ${{ secrets.GH_SA_TOKEN }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          machine_zone: ${{ vars.GCP_ZONE }}
          machine_type: n4-highcpu-8
          network: gh-runner
          runner_label: micro-benchmark-${{ github.run_id }}-${{ github.run_number }}

  run-micro-benchmarks:
    needs:
      - create-runner
    runs-on: micro-benchmark-${{ github.run_id }}-${{ github.run_number }}
    container: falkordb/falkordb-build:ubuntu
    steps:
      - name: Safe dir
        run: git config --global --add safe.directory '*'

      - uses: actions/checkout@v6
        with:
          set-safe-directory: '*'
          submodules: recursive

      - name: Cache GraphBLAS
        id: cache_graphblas
        uses: actions/cache@v5
        with:
          path: ./bin/linux-x64-release/GraphBLAS
          key: graphblas-x64-${{ hashFiles('./deps/GraphBLAS/Include/GraphBLAS.h', './deps/GraphBLAS/Config/GB_prejit.c', './build.sh') }}

      - name: Cache parser
        id: cache_parser
        uses: actions/cache@v5
        with:
          path: ./bin/linux-x64-release/libcypher-parser
          key: parser-x64-${{ hashFiles('./deps/libcypher-parser/lib/src/parser.c', './build.sh') }}

      - name: Cache search
        id: cache_search
        uses: actions/cache@v5
        with:
          path: ./bin/linux-x64-release/search-static
          key: search-x64-${{ hashFiles('./deps/RediSearch/src/version.h', './build.sh') }}

      - name: Cache libcurl
        id: cache_libcurl
        uses: actions/cache@v5
        with:
          path: ./bin/linux-x64-release/libcurl
          key: libcurl-x64-${{ hashFiles('./deps/libcurl/RELEASE-NOTES', './build.sh') }}

      - name: Cache libcsv
        id: cache_libcsv
        uses: actions/cache@v5
        with:
          path: ./bin/linux-x64-release/libcsv
          key: libcsv-x64-${{ hashFiles('./deps/libcsv/ChangeLog', './build.sh') }}

      - name: Install Google Benchmark
        run: |
          apt update && apt install -y libbenchmark-dev

      - name: Build with micro-benchmarks
        run: |
          rustup default stable
          BUILD_BENCHMARKS=1 make -j$(nproc)

      - name: Run micro-benchmarks
        run: make micro-benchmark

      - name: Upload results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: ${{ github.head_ref || github.ref_name }}-micro-benchmark-results
          path: tests/micro_benchmarks/results/*_results.json
          retention-days: 30

  cleanup-runner:
    needs: run-micro-benchmarks
    if: always()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
      - uses: ./.github/actions/cleanup-runner
        with:
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          zone: ${{ vars.GCP_ZONE }}
          instance_label: micro-benchmark-${{ github.run_id }}-${{ github.run_number }}

  compare-results:
    needs: run-micro-benchmarks
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Create comparison directories
        run: mkdir -p compare/pr compare/master

      - name: Download PR benchmark results
        uses: actions/download-artifact@v7
        with:
          name: ${{ github.head_ref }}-micro-benchmark-results
          path: compare/pr

      - name: Download master benchmark results
        uses: actions/github-script@v8
        with:
          script: |
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'master-micro-benchmark-results',
              per_page: 1
            })
            if (artifacts.data.total_count === 0) {
              core.warning('No master micro-benchmark results found â€” comparison will be skipped.')
              return
            }
            const download_res = await github.rest.actions.downloadArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifacts.data.artifacts[0].id,
              archive_format: 'zip',
            })
            require('fs').writeFileSync('master-results.zip', Buffer.from(download_res.data))
        continue-on-error: true

      - name: Unzip master results
        run: unzip master-results.zip -d compare/master
        continue-on-error: true

      - name: Install comparison dependencies
        working-directory: tests/micro_benchmarks
        run: pip install -r comparison_requirements.txt

      - name: Generate comparison markdown
        working-directory: tests/micro_benchmarks
        run: |
          python3 compare_results.py \
            --master_dir ../../compare/master \
            --branch_dir ../../compare/pr \
            --branch_name "${{ github.head_ref }}" \
            --output ../../compare/micro_benchmark_compare.md
        continue-on-error: true

      - name: Post comparison to PR
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs')
            let comparison = '_Micro-benchmark comparison could not be generated._'
            try {
              comparison = fs.readFileSync('compare/micro_benchmark_compare.md', 'utf-8')
            } catch (e) {
              core.warning(`Could not read comparison file: ${e}`)
            }

            const existing_pull = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.payload.pull_request.number,
              headers: { 'Accept': 'application/vnd.github-json' }
            })

            const startMarker = '<!-- Micro-benchmark results auto generation start -->'
            const endMarker   = '<!-- Micro-benchmark results auto generation end -->'
            const regex       = new RegExp(`${startMarker}[\\s\\S]*?${endMarker}`, 'g')
            const section     = `${startMarker}\n${comparison}\n${endMarker}`

            let body = existing_pull.data.body || ''
            body = body.includes(startMarker)
              ? body.replace(regex, section)
              : `${body}\n${section}\n`

            await github.rest.pulls.update({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.payload.pull_request.number,
              body: body,
            })

  push-to-grafana:
    needs: run-micro-benchmarks
    if: ${{ github.event_name != 'pull_request' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Download benchmark results
        uses: actions/download-artifact@v7
        with:
          name: ${{ github.ref_name }}-micro-benchmark-results
          path: tests/micro_benchmarks/results

      - name: Install Grafana push dependencies
        run: pip install -r tests/micro_benchmarks/grafana_requirements.txt

      - name: Push results to Grafana
        env:
          GRAFANA_REMOTE_WRITE_URL: ${{ vars.GRAFANA_REMOTE_WRITE_URL }}
          GRAFANA_METRICS_USER:     ${{ secrets.GRAFANA_METRICS_USER }}
          GRAFANA_METRICS_TOKEN:    ${{ secrets.GRAFANA_METRICS_TOKEN }}
        run: |
          python3 tests/micro_benchmarks/push_to_grafana.py \
            --branch      "${{ github.ref_name }}" \
            --commit-sha  "${{ github.sha }}" \
            --event-name  "${{ github.event_name }}"
